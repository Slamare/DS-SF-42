{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "- Basics of NLP: tokenization, stopwords, POS tagging, stemming/lematization\n",
    "- TextBlob library. How to process text with it and do sentiment analysis\n",
    "- Text classification in sklearn: vectorizing text, modeling with naive bayes, and model optimization with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language\n",
    "- Also referred to as machine learning with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [A friend's application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tools\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP is hard! Here's why\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages, \"y r u\" vs \"why are you\"\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\", \"clickbait\", \"fleek\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "- **Texts with the same words and phrases can having different meanings **: \n",
    "State farm commercial where two different people say \"Is this my car? What? This is ridiculous! This can't be happening! Shut up! Ahhhh!!!\"\n",
    "\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with the NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NLTK should be installed and its additional materials should be downloaded as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Downloads the nltk data\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello.',\n",
       " 'How are you, dear Mr. Sir?',\n",
       " 'Are you well?',\n",
       " 'Here: drink this!',\n",
       " 'It will make you feel better.',\n",
       " \"I mean, it won't make you feel worse!\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Hello. How are you, dear Mr. Sir? Are you well?\n",
    "          Here: drink this! It will make you feel better.\n",
    "          I mean, it won't make you feel worse!\"\"\"\n",
    "\n",
    "\n",
    "#Tokenize text using sent_tokenize function\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, can you figure out the rules of tokenization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean, it won't make you feel worse!\n",
      "['I', 'mean', ',', 'it', 'wo', \"n't\", 'make', 'you', 'feel', 'worse', '!']\n"
     ]
    }
   ],
   "source": [
    "#Assign last sentence in sentences to sentence\n",
    "\n",
    "sentence = sentences[5]\n",
    "\n",
    "\n",
    "#Word tokenize using one of the sentences from sentences\n",
    "#Assumes that input has already been tokenized into sentences\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the word_tokenize function work? Let's try the wordpunct_tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'mean', ',', 'it', 'won', \"'\", 't', 'make', 'you', 'feel', 'worse', '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass sentence into wordpunct_tokenize function\n",
    "wordpunct_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online demo of various tokenizers: http://text-processing.com/demo/tokenize/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging\n",
    "\n",
    "<br>\n",
    "\n",
    "\"The process of assigning one of the parts of speech to the given word is called Parts Of Speech tagging. It is commonly referred to as POS tagging. Parts of speech include nouns, verbs, adverbs, adjectives, pronouns, conjunction and their sub-categories.\"\n",
    "\n",
    "http://language.worldofcomputing.net/pos-tagging/parts-of-speech-tagging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('assigning', 'VBG'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('parts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('given', 'VBN'),\n",
       " ('word', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('called', 'VBN'),\n",
       " ('Parts', 'NNS'),\n",
       " ('Of', 'IN'),\n",
       " ('Speech', 'NNP'),\n",
       " ('tagging', 'VBG')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text for POS tagging\n",
    "text = \"\"\"The process of assigning one of \n",
    "the parts of speech to the given word is called Parts Of Speech tagging\"\"\"\n",
    "\n",
    "#Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#Pass tokens into pos_tag function\n",
    "pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is tuple pairings of tokens with their POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of POS tags: \n",
    "WP: wh-pronoun (\"who\", \"what\")  \n",
    "VBZ: verb, 3rd person sing. present (\"takes\")  \n",
    "VBG: verb, gerund/present participle (\"taking\")  \n",
    "TO: to (\"to go\", \"to him\")   \n",
    "DT: determiner (\"the\", \"this\")  \n",
    "NN: noun, singular or mass (\"door\")  \n",
    "\n",
    "All tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Common words that will likely appear in any text. Anything that can appears in a poem, rap lyric, or medical research paper is most likely a stopword. In most NLP contexts, we remove the stopwords because they don't tell you much about your text, they have no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Intialize the list of stopwords \n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View list of punctuation characters\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Add them to the sw list\n",
    "sw += punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove stopwords and punctuation from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = \"\"\"Sony Michel's touchdown in double-overtime gave \n",
    "Georgia a 54-48 Rose Bowl win over Oklahoma and \n",
    "made up for a late fumble that resulted in six points for the Sooners.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sony',\n",
       " 'Michel',\n",
       " \"'\",\n",
       " 's',\n",
       " 'touchdown',\n",
       " 'in',\n",
       " 'double',\n",
       " '-',\n",
       " 'overtime',\n",
       " 'gave',\n",
       " 'Georgia',\n",
       " 'a',\n",
       " '54',\n",
       " '-',\n",
       " '48',\n",
       " 'Rose',\n",
       " 'Bowl',\n",
       " 'win',\n",
       " 'over',\n",
       " 'Oklahoma',\n",
       " 'and',\n",
       " 'made',\n",
       " 'up',\n",
       " 'for',\n",
       " 'a',\n",
       " 'late',\n",
       " 'fumble',\n",
       " 'that',\n",
       " 'resulted',\n",
       " 'in',\n",
       " 'six',\n",
       " 'points',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Sooners',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize text\n",
    "\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sony',\n",
       " 'Michel',\n",
       " 'touchdown',\n",
       " 'double',\n",
       " 'overtime',\n",
       " 'gave',\n",
       " 'Georgia',\n",
       " '54',\n",
       " '48',\n",
       " 'Rose',\n",
       " 'Bowl',\n",
       " 'win',\n",
       " 'Oklahoma',\n",
       " 'made',\n",
       " 'late',\n",
       " 'fumble',\n",
       " 'resulted',\n",
       " 'six',\n",
       " 'points',\n",
       " 'Sooners']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean up tokens by removing stopwords and punctuation characters\n",
    "\n",
    "clean_tokens = [i for i in tokens if i not in sw]\n",
    "\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize stemmer object\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Derive stems from random words\n",
    "\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolut'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"absolutely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forgav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"forgave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Sony',\n",
       "  'Michel',\n",
       "  'touchdown',\n",
       "  'double',\n",
       "  'overtime',\n",
       "  'gave',\n",
       "  'Georgia',\n",
       "  '54',\n",
       "  '48',\n",
       "  'Rose',\n",
       "  'Bowl',\n",
       "  'win',\n",
       "  'Oklahoma',\n",
       "  'made',\n",
       "  'late',\n",
       "  'fumble',\n",
       "  'resulted',\n",
       "  'six',\n",
       "  'points',\n",
       "  'Sooners'],\n",
       " ['soni',\n",
       "  'michel',\n",
       "  'touchdown',\n",
       "  'doubl',\n",
       "  'overtim',\n",
       "  'gave',\n",
       "  'georgia',\n",
       "  '54',\n",
       "  '48',\n",
       "  'rose',\n",
       "  'bowl',\n",
       "  'win',\n",
       "  'oklahoma',\n",
       "  'made',\n",
       "  'late',\n",
       "  'fumbl',\n",
       "  'result',\n",
       "  'six',\n",
       "  'point',\n",
       "  'sooner'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Derive the stems of every token in clean tokens\n",
    "stems = [stemmer.stem(token) for token in clean_tokens]\n",
    "\n",
    "clean_tokens, stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the results of the stemming process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the stems and lemmatization of certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopi'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stem of octopi (plural of octopus)\n",
    "\n",
    "stemmer.stem(\"octopi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize lemmatization object\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatize octopi\n",
    "\n",
    "lem.lemmatize(\"octopi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? Try it again with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indic'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stem\n",
    "\n",
    "stemmer.stem(\"indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemon\n",
    "lem.lemmatize(\"indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the lemons of clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sony',\n",
       " 'Michel',\n",
       " 'touchdown',\n",
       " 'double',\n",
       " 'overtime',\n",
       " 'give',\n",
       " 'Georgia',\n",
       " '54',\n",
       " '48',\n",
       " 'Rose',\n",
       " 'Bowl',\n",
       " 'win',\n",
       " 'Oklahoma',\n",
       " 'make',\n",
       " 'late',\n",
       " 'fumble',\n",
       " 'result',\n",
       " 'six',\n",
       " 'point',\n",
       " 'Sooners']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmative the clean tokens and set pos = v\n",
    "\n",
    "lemons = [lem.lemmatize(token, pos= \"v\") for token in clean_tokens]\n",
    "\n",
    "lemons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "Collections of adjacent words, number of words in each collection is determined by N. \n",
    "\n",
    "Bigrams = Two-word phrases\n",
    "\n",
    "Trigrams = Three-word phrases\n",
    "\n",
    "http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x11d6d2888>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Set N to 2 for bigrams\n",
    "N = 2\n",
    "\n",
    "#Make bigrams from clean_tokens\n",
    "bigrams = ngrams(clean_tokens, N)\n",
    "\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x1216bffc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Set N to 3 for trigrams\n",
    "N = 3\n",
    "\n",
    "#Make bigrams from clean_tokens\n",
    "trigrams = ngrams(clean_tokens, N)\n",
    "\n",
    "trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "<br>\n",
    "\n",
    "Python library for processing simple NLP tasks.\n",
    "\n",
    "\n",
    "You may need to download the corpora in textblob. Type into command line:\n",
    "\n",
    "python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text for using TextBlob\n",
    "\n",
    "corpus = \"\"\"\n",
    "Mr. Persson, 35, sits in front of four computer screens,\n",
    "one displaying the loader he steers as it lifts freshly blasted rock containing silver,\n",
    "zinc and lead. If he were down in the mine shaft operating the loader manually,\n",
    "he would be inhaling dust and exhaust fumes. \n",
    "Instead, he reclines in an office chair while using a joystick to control the machine.\n",
    "\"\"\"\n",
    "\n",
    "#Pass in text into textblob\n",
    "blob = TextBlob(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the capabilities of textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mr', 'Persson', '35', 'sits', 'in', 'front', 'of', 'four', 'computer', 'screens', 'one', 'displaying', 'the', 'loader', 'he', 'steers', 'as', 'it', 'lifts', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fumes', 'Instead', 'he', 'reclines', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized words\n",
    "\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " Mr. Persson, 35, sits in front of four computer screens,\n",
       " one displaying the loader he steers as it lifts freshly blasted rock containing silver,\n",
       " zinc and lead.\"),\n",
       " Sentence(\"If he were down in the mine shaft operating the loader manually,\n",
       " he would be inhaling dust and exhaust fumes.\"),\n",
       " Sentence(\"Instead, he reclines in an office chair while using a joystick to control the machine.\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentences\n",
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'35': 1,\n",
       "             'a': 1,\n",
       "             'an': 1,\n",
       "             'and': 2,\n",
       "             'as': 1,\n",
       "             'be': 1,\n",
       "             'blasted': 1,\n",
       "             'chair': 1,\n",
       "             'computer': 1,\n",
       "             'containing': 1,\n",
       "             'control': 1,\n",
       "             'displaying': 1,\n",
       "             'down': 1,\n",
       "             'dust': 1,\n",
       "             'exhaust': 1,\n",
       "             'four': 1,\n",
       "             'freshly': 1,\n",
       "             'front': 1,\n",
       "             'fumes': 1,\n",
       "             'he': 4,\n",
       "             'if': 1,\n",
       "             'in': 3,\n",
       "             'inhaling': 1,\n",
       "             'instead': 1,\n",
       "             'it': 1,\n",
       "             'joystick': 1,\n",
       "             'lead': 1,\n",
       "             'lifts': 1,\n",
       "             'loader': 2,\n",
       "             'machine': 1,\n",
       "             'manually': 1,\n",
       "             'mine': 1,\n",
       "             'mr': 1,\n",
       "             'of': 1,\n",
       "             'office': 1,\n",
       "             'one': 1,\n",
       "             'operating': 1,\n",
       "             'persson': 1,\n",
       "             'reclines': 1,\n",
       "             'rock': 1,\n",
       "             'screens': 1,\n",
       "             'shaft': 1,\n",
       "             'silver': 1,\n",
       "             'sits': 1,\n",
       "             'steers': 1,\n",
       "             'the': 4,\n",
       "             'to': 1,\n",
       "             'using': 1,\n",
       "             'were': 1,\n",
       "             'while': 1,\n",
       "             'would': 1,\n",
       "             'zinc': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word counts\n",
    "\n",
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Persson', 'NNP'),\n",
       " ('35', 'CD'),\n",
       " ('sits', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('front', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('four', 'CD'),\n",
       " ('computer', 'NN'),\n",
       " ('screens', 'NNS'),\n",
       " ('one', 'CD'),\n",
       " ('displaying', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('loader', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('steers', 'VBZ'),\n",
       " ('as', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('lifts', 'VBZ'),\n",
       " ('freshly', 'RB'),\n",
       " ('blasted', 'VBN'),\n",
       " ('rock', 'NN'),\n",
       " ('containing', 'VBG'),\n",
       " ('silver', 'NN'),\n",
       " ('zinc', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('lead', 'NN'),\n",
       " ('If', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('down', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mine', 'NN'),\n",
       " ('shaft', 'NN'),\n",
       " ('operating', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('loader', 'NN'),\n",
       " ('manually', 'RB'),\n",
       " ('he', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('inhaling', 'VBG'),\n",
       " ('dust', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('exhaust', 'JJ'),\n",
       " ('fumes', 'NNS'),\n",
       " ('Instead', 'RB'),\n",
       " ('he', 'PRP'),\n",
       " ('reclines', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('office', 'NN'),\n",
       " ('chair', 'NN'),\n",
       " ('while', 'IN'),\n",
       " ('using', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('joystick', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('control', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('machine', 'NN')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pos tags\n",
    "\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['mr. persson', 'computer screens', 'mine shaft', 'exhaust fumes', 'office chair'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Noun phrases\n",
    "\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mr', 'Persson', '35', 'sit', 'in', 'front', 'of', 'fmy', 'computer', 'screen', 'one', 'displaying', 'the', 'loader', 'he', 'steer', 'a', 'it', 'lift', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fume', 'Instead', 'he', 'recline', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Singularize words\n",
    "blob.words.singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mrs', 'Perssons', '35s', 'sitss', 'ins', 'fronts', 'ofs', 'fours', 'computers', 'screenss', 'ones', 'displayings', 'thes', 'loaders', 'they', 'steerss', 'ass', 'they', 'liftss', 'freshlies', 'blasteds', 'rocks', 'containings', 'silvers', 'zincs', 'ands', 'leads', 'Ifs', 'they', 'weres', 'downs', 'ins', 'thes', 'ours', 'shafts', 'operatings', 'thes', 'loaders', 'manuallies', 'they', 'woulds', 'bes', 'inhalings', 'dusts', 'ands', 'exhausts', 'fumess', 'Insteads', 'they', 'recliness', 'ins', 'some', 'offices', 'chairs', 'whiles', 'usings', 'some', 'joysticks', 'toes', 'controls', 'thes', 'machines'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pluralize words\n",
    "blob.words.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mr', 'Persson', '35', 'sits', 'in', 'front', 'of', 'four', 'computer', 'screens', 'one', 'displaying', 'the', 'loader', 'he', 'steers', 'as', 'it', 'lifts', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fumes', 'Instead', 'he', 'reclines', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-f673d7033d0b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-f673d7033d0b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print [word.lemmatize() for word in blob.words]\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "print [word.lemmatize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization with verbs\n",
    "print [word.lemmatize(pos = \"v\") for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Mr', 'Persson']),\n",
       " WordList(['Persson', '35']),\n",
       " WordList(['35', 'sits']),\n",
       " WordList(['sits', 'in']),\n",
       " WordList(['in', 'front']),\n",
       " WordList(['front', 'of']),\n",
       " WordList(['of', 'four']),\n",
       " WordList(['four', 'computer']),\n",
       " WordList(['computer', 'screens']),\n",
       " WordList(['screens', 'one']),\n",
       " WordList(['one', 'displaying']),\n",
       " WordList(['displaying', 'the']),\n",
       " WordList(['the', 'loader']),\n",
       " WordList(['loader', 'he']),\n",
       " WordList(['he', 'steers']),\n",
       " WordList(['steers', 'as']),\n",
       " WordList(['as', 'it']),\n",
       " WordList(['it', 'lifts']),\n",
       " WordList(['lifts', 'freshly']),\n",
       " WordList(['freshly', 'blasted']),\n",
       " WordList(['blasted', 'rock']),\n",
       " WordList(['rock', 'containing']),\n",
       " WordList(['containing', 'silver']),\n",
       " WordList(['silver', 'zinc']),\n",
       " WordList(['zinc', 'and']),\n",
       " WordList(['and', 'lead']),\n",
       " WordList(['lead', 'If']),\n",
       " WordList(['If', 'he']),\n",
       " WordList(['he', 'were']),\n",
       " WordList(['were', 'down']),\n",
       " WordList(['down', 'in']),\n",
       " WordList(['in', 'the']),\n",
       " WordList(['the', 'mine']),\n",
       " WordList(['mine', 'shaft']),\n",
       " WordList(['shaft', 'operating']),\n",
       " WordList(['operating', 'the']),\n",
       " WordList(['the', 'loader']),\n",
       " WordList(['loader', 'manually']),\n",
       " WordList(['manually', 'he']),\n",
       " WordList(['he', 'would']),\n",
       " WordList(['would', 'be']),\n",
       " WordList(['be', 'inhaling']),\n",
       " WordList(['inhaling', 'dust']),\n",
       " WordList(['dust', 'and']),\n",
       " WordList(['and', 'exhaust']),\n",
       " WordList(['exhaust', 'fumes']),\n",
       " WordList(['fumes', 'Instead']),\n",
       " WordList(['Instead', 'he']),\n",
       " WordList(['he', 'reclines']),\n",
       " WordList(['reclines', 'in']),\n",
       " WordList(['in', 'an']),\n",
       " WordList(['an', 'office']),\n",
       " WordList(['office', 'chair']),\n",
       " WordList(['chair', 'while']),\n",
       " WordList(['while', 'using']),\n",
       " WordList(['using', 'a']),\n",
       " WordList(['a', 'joystick']),\n",
       " WordList(['joystick', 'to']),\n",
       " WordList(['to', 'control']),\n",
       " WordList(['control', 'the']),\n",
       " WordList(['the', 'machine'])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "TextBlob uses an algorithm to rate text on subjectivity and polarity. Subjectivity measures how opinonated a text is on a scale from 0.0-1.0 and polarity measures how happy or mad or a text is on a scale from -1.0-1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.445, subjectivity=0.43)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text for sentiment analysis\n",
    "raw_text = \"I love learning about data science, it is very fun.\"\n",
    "\n",
    "#Pass in raw_text into textblob\n",
    "blob = TextBlob(raw_text)\n",
    "\n",
    "#Derive scores\n",
    "blob.sentiment\n",
    "\n",
    "#polarity: -1 negative sentiment to 1 (positive)\n",
    "#sentiment: 0 (objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polarity score\n",
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subjectivity score\n",
    "blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"it's so awesome\").sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I love this course.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Oh my god I love this course.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"it's so awesome.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.8, subjectivity=0.9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I hate cupcakes.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"i have no opinions about the matter\").sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the sentiment of yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\r\\n\\r\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\r\\n\\r\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read first review\n",
    "\n",
    "review = yelp.text[0]\n",
    "\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.40246913580246907, subjectivity=0.6591122868900646)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Textblob review and get its sentiments scores\n",
    "\n",
    "blob = TextBlob(review)\n",
    "\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the scores? Are they too high or low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate polarity and subjectivity scores for entire corpus\n",
    "# by applying polarity and sentiment over yelp reviews df\n",
    "\n",
    "yelp[\"polarity\"] = yelp.text.apply(lambda x:TextBlob(x).polarity)\n",
    "yelp[\"subjectivity\"] = yelp.text.apply(lambda x:TextBlob(x).subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most negative and positives reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adjust settings\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the supreme pizza with the mystery meats.  I threw it in the trash. I will wait until I get to my destination to eat. Horrible!!!\n",
       "1517                                                                                                                                      Nasty workers and over priced trash\n",
       "3266                                                                                                         Absolutely awful... these guys have NO idea what they are doing!\n",
       "4766                                                                                                                                                           Very bad food!\n",
       "5812                                                                                                                            I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most negative\n",
    "\n",
    "yelp[yelp.polarity == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254                                                                                                                                                                             Our server Gary was awesome. Food was amazing...an experience.\n",
       "347                                                                                                                                                           3 syllables for this place. \\r\\nA-MAZ-ING!\\r\\n\\r\\nThe best Phoenix has to offer.\n",
       "420                                                                                                                                                                                                                          LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as Chino is the one thing we miss. Every time I think about Chino Bandido my mouth starts watering. If I am ever in the state again I will drive out of my way just to go to it again. YUMMY!\n",
       "679                                                                                                                                                                                                                           Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most positive\n",
    "\n",
    "yelp[yelp.polarity == 1].text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there reviews with 5 stars but low polarity scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390     RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!\n",
       "1287                                             Obsessed. Like, I've-got-the-Twangy-Tart-withdrawal-shakes level of addiction to this place. Please make one in Arcadia! Pleeeaaassse.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "yelp[(yelp.stars == 5) & (yelp.polarity < -0.3)][\"text\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1781                                                                                                                                                                                                                                                                     If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.\n",
       "2353    My co-workers and I refer to this place as \"Pizza n' Ants\".  The staff will be happy to serve you with bare hands, right after using the till.  Also, as the nickname suggests, there has been a noticable insect problem. \\r\\r\\n\\r\\r\\nAs if that could all be overlooked, the pizza isn't even good.  If you are in this part of town, go to Z Pizza or Slices for great pizza instead!\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One star reviews with high polarity scores\n",
    "yelp[(yelp.stars == 1) & (yelp.polarity > 0.5)][\"text\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAD8CAYAAACPWyg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSerM8JAkdfb/ARUqgo1S+6m8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123204438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of polarity scores\n",
    "\n",
    "yelp.polarity.plot(kind=\"hist\", bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1235c1eb8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD8CAYAAABthzNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEXVJREFUeJzt3XuMZnV9x/H3B1ABr1Bwiwu4aFYtthpxpdRbtWhQqS42\nlpJ6IcZIG6mXtrYsxKhJs8matkZti3WLtnglqFTWorYr9dLGCy6KcitlK7eFRVbbiqIBF7794zmr\nI/3NzJllzvM8M/N+JZM51+f5/jKT5/P8zvmdc1JVSJJ0b/tMugBJ0nQyICRJTQaEJKnJgJAkNRkQ\nkqQmA0KS1GRASJKaDAhJUpMBIUlq2m/SBdwXhxxySK1Zs2bSZUjSknLppZd+t6oOnW+7JR0Qa9as\nYdu2bZMuQ5KWlCQ39NnOQ0ySpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqSm\nJX0ltbRSrdlw0azrrt904hgr0XJmQEgTMNcHvDQtPMQkSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS\n1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktTk7b6lgXhLby119iAkSU0GhCSp\nyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqWnQgEjyh0muTHJFko8k2T/JwUm2\nJrm2+33QjO3PTLI9yTVJThiyNknS3AYLiCSrgdcB66rql4F9gVOADcDFVbUWuLibJ8nR3frHA88D\nzk6y71D1SZLmNvQhpv2AA5LsBxwI3AKsB87t1p8LnNRNrwfOq6o7q+o6YDtw7MD1SZJmMVhAVNXN\nwF8ANwI7ge9X1b8Aq6pqZ7fZrcCqbno1cNOMl9jRLZMkTcCQh5gOYtQrOAp4BPDAJC+buU1VFVAL\nfN3TkmxLsm3Xrl2LVq8k6ecNeYjpOcB1VbWrqn4CXAA8FfhOksMAut+3ddvfDBwxY//Du2U/p6o2\nV9W6qlp36KGHDli+JK1sQwbEjcBxSQ5MEuB44GpgC3Bqt82pwIXd9BbglCQPSHIUsBa4ZMD6JElz\nGOyJclX11SQfA74O7Aa+AWwGHgScn+RVwA3Ayd32VyY5H7iq2/70qrp7qPokSXMb9JGjVfUW4C33\nWnwno95Ea/uNwMYha5Ik9eOV1JKkJgNCktRkQEiSmgwISVKTASFJajIgJElNgw5zlTR+azZcNOf6\n6zedOKZKtNTZg5AkNRkQkqQmA0KS1GRASJKaDAhJUpOjmKS9NN9oIWmpswchSWoyICRJTQaEJKnJ\ngJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwI\nSVKTDwySVpi5HnR0/aYTx1iJpp09CElSkz0IaQ4+VlQrmT0ISVKTASFJajIgJElNgwZEkocl+ViS\n/0hydZJfS3Jwkq1Jru1+HzRj+zOTbE9yTZIThqxNkjS3oXsQ7wQ+U1WPA54IXA1sAC6uqrXAxd08\nSY4GTgEeDzwPODvJvgPXJ0maxWABkeShwDOB9wJU1V1V9b/AeuDcbrNzgZO66fXAeVV1Z1VdB2wH\njh2qPknS3HoFRJJf2YvXPgrYBfx9km8kOSfJA4FVVbWz2+ZWYFU3vRq4acb+O7plkqQJ6NuDODvJ\nJUle0/UM+tgPOAZ4d1U9CbiD7nDSHlVVQPWuFkhyWpJtSbbt2rVrIbtKkhagV0BU1TOAlwJHAJcm\n+XCS586z2w5gR1V9tZv/GKPA+E6SwwC637d162/uXn+Pw7tl965lc1Wtq6p1hx56aJ/yJUl7ofc5\niKq6FngTcAbw68C7utFJvzXL9rcCNyV5bLfoeOAqYAtwarfsVODCbnoLcEqSByQ5ClgLXLLA9kiS\nFkmvW20keQLwSuBEYCvwwqr6epJHAF8GLphl19cCH0pyf+Db3WvsA5yf5FXADcDJAFV1ZZLzGYXI\nbuD0qrp7r1smSbpP+t6L6a+Ac4CzqurHexZW1S1J3jTbTlV1GbCuser4WbbfCGzsWZMkaUB9A+JE\n4Md7vtEn2QfYv6p+VFUfGKw6SdLE9D0H8VnggBnzB3bLJEnLVN+A2L+qfrhnpps+cJiSJEnToG9A\n3JHkmD0zSZ4M/HiO7SVJS1zfcxBvAD6a5BYgwC8CvzNYVZKkiesVEFX1tSSPA/Zc03BNVf1kuLIk\nSZO2kEeOPgVY0+1zTBKq6v2DVCVJmri+F8p9AHg0cBmw5+K1AgwISVqm+vYg1gFHdzfXkyStAH1H\nMV3B6MS0JGmF6NuDOAS4KsklwJ17FlbViwapSpI0cX0D4q1DFiFJmj59h7l+IckjgbVV9dkkBwI+\nL1paZtZsuGjO9ddvOnFMlWga9H3k6KsZPfDnPd2i1cAnhipKkjR5fU9Snw48DbgdfvrwoIcPVZQk\nafL6BsSdVXXXnpkk+7HAZ0lLkpaWvgHxhSRnAQd0z6L+KPDJ4cqSJE1a34DYAOwCLgd+D/gUo+dT\nS5KWqb6jmO4B/q77kSStAH3vxXQdjXMOVfWoRa9IkjQVFnIvpj32B34bOHjxy5EkTYte5yCq6nsz\nfm6uqncAXjEjSctY30NMx8yY3YdRj2Ihz5KQJC0xfT/k/3LG9G7geuDkRa9GkjQ1+o5ievbQhUiS\npkvfQ0x/NNf6qnr74pQjSZoWCxnF9BRgSzf/QuAS4NohipIkTV7fgDgcOKaqfgCQ5K3ARVX1sqEK\nkyRNVt9bbawC7poxf1e3TJK0TPXtQbwfuCTJP3bzJwHnDlOSJGka9B3FtDHJp4FndIteWVXfGK4s\naTzme4KatJL1PcQEcCBwe1W9E9iR5KiBapIkTYG+jxx9C3AGcGa36H7AB4cqSpI0eX17EC8GXgTc\nAVBVtwAPHqooSdLk9Q2Iu6qq6G75neSBw5UkSZoGfQPi/CTvAR6W5NXAZ+n58KAk+yb5RpJ/6uYP\nTrI1ybXd74NmbHtmku1JrklywkIbI0laPH1HMf1F9yzq24HHAm+uqq093+P1wNXAQ7r5DcDFVbUp\nyYZu/owkRwOnAI8HHgF8Nsljquru/s2RNKT5Rn1dv8mnACwn8/Yguh7A56pqa1X9SVW9sW84JDmc\n0XMjzpmxeD0/u4biXEbXVOxZfl5V3VlV1wHbgWP7NkSStLjmDYjuG/w9SR66F6//DuBPgXtmLFtV\nVTu76Vv52RXZq4GbZmy3o1smSZqAvldS/xC4PMlWupFMAFX1utl2SPKbwG1VdWmSZ7W2qapK8v+e\ndT2XJKcBpwEceeSRC9lVkrQAfQPigu5nIZ4GvCjJCxg9x/ohST4IfCfJYVW1M8lhwG3d9jcDR8zY\n//Bu2c+pqs3AZoB169YtKFwkSf3NGRBJjqyqG6tqwfddqqoz6S6s63oQb6yqlyX5c+BUYFP3+8Ju\nly3Ah5O8ndFJ6rWMbikuSZqA+c5BfGLPRJKPL9J7bgKem+Ra4DndPFV1JXA+cBXwGeB0RzBJ0uTM\nd4gpM6YftbdvUlWfBz7fTX8POH6W7TYCG/f2fSRJi2e+HkTNMi1JWubm60E8McntjHoSB3TTdPNV\nVQ+ZfVdJ0lI2Z0BU1b7jKkSSNF0W8jwISdIKYkBIkpr6XignLUk+UlTae/YgJElNBoQkqcmAkCQ1\nGRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMB\nIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmvabdAGSlo81Gy6add31m04cYyVaDPYgJElN\nBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoGC4gkRyT5XJKrklyZ5PXd8oOTbE1ybff7oBn7nJlke5Jr\nkpwwVG2SpPkN2YPYDfxxVR0NHAecnuRoYANwcVWtBS7u5unWnQI8HngecHaSfQesT5I0h8ECoqp2\nVtXXu+kfAFcDq4H1wLndZucCJ3XT64HzqurOqroO2A4cO1R9kqS5jeVK6iRrgCcBXwVWVdXObtWt\nwKpuejXwlRm77eiWSXOa6+pdTY/5/k5eaT19Bj9JneRBwMeBN1TV7TPXVVUBtcDXOy3JtiTbdu3a\ntYiVSpJmGrQHkeR+jMLhQ1V1Qbf4O0kOq6qdSQ4DbuuW3wwcMWP3w7tlP6eqNgObAdatW7egcJE0\nvbyP0/QZchRTgPcCV1fV22es2gKc2k2fClw4Y/kpSR6Q5ChgLXDJUPVJkuY2ZA/iacDLgcuTXNYt\nOwvYBJyf5FXADcDJAFV1ZZLzgasYjYA6varuHrA+SdIcBguIqvp3ILOsPn6WfTYCG4eqSZLUn1dS\nS5KaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkprHcrE+6L7wZnzQZ9iAkSU0GhCSpyYCQJDUZ\nEJKkJgNCktRkQEiSmhzmqolzGKs0nexBSJKaDAhJUpMBIUlq8hyEpKk333mq6zedOKZKVhZ7EJKk\nJnsQkpY8exjDsAchSWpa0T2Iub51+I1jYfwGJy0/9iAkSU0GhCSpyYCQJDWt6HMQGh/vt6RJ8nzj\n3jEgJGkKTcPADw8xSZKaDAhJUpOHmNSb5xGklcUehCSpyR6EJM1hJY+AMiAkrWj35dDpNIw0GpIB\nsYIs939mSYtr6s5BJHlekmuSbE+yYdL1SNJKNVU9iCT7An8DPBfYAXwtyZaqumqylU2PIXsBjlKS\nFtdS77VPVUAAxwLbq+rbAEnOA9YDyyog/CCWBNP/WTBtAbEauGnG/A7gVydUy5ym9Q87rXVJWnqm\nLSDmleQ04LRu9odJrrkPL3cI8N3m+7ztPrzq9Jq1vcuYbV4ZVlyb87b71OZH9tlo2gLiZuCIGfOH\nd8t+qqo2A5sX482SbKuqdYvxWkvBSmsv2OaVwjYPY9pGMX0NWJvkqCT3B04Btky4JklakaaqB1FV\nu5P8AfDPwL7A+6rqygmXJUkr0lQFBEBVfQr41JjeblEOVS0hK629YJtXCts8gFTV0O8hSVqCpu0c\nhCRpSiz7gJjv1h0ZeVe3/ltJjplEnYupR5tf2rX18iRfSvLESdS5mPreoiXJU5LsTvKScdY3hD5t\nTvKsJJcluTLJF8Zd42Lr8b/90CSfTPLNrs2vnESdiyXJ+5LcluSKWdYP+/lVVcv2h9GJ7v8CHgXc\nH/gmcPS9tnkB8GkgwHHAVydd9xja/FTgoG76+SuhzTO2+1dG57heMum6x/B3fhijuxAc2c0/fNJ1\nj6HNZwFv66YPBf4buP+ka78PbX4mcAxwxSzrB/38Wu49iJ/euqOq7gL23LpjpvXA+2vkK8DDkhw2\n7kIX0bxtrqovVdX/dLNfYXS9yVLW5+8M8Frg48Bt4yxuIH3a/LvABVV1I0BVLfV292lzAQ9OEuBB\njAJi93jLXDxV9UVGbZjNoJ9fyz0gWrfuWL0X2ywlC23Pqxh9A1nK5m1zktXAi4F3j7GuIfX5Oz8G\nOCjJ55NcmuQVY6tuGH3a/NfALwG3AJcDr6+qe8ZT3kQM+vk1dcNcNT5Jns0oIJ4+6VrG4B3AGVV1\nz+jL5YqwH/Bk4HjgAODLSb5SVf852bIGdQJwGfAbwKOBrUn+rapun2xZS9NyD4h5b93Rc5ulpFd7\nkjwBOAd4flV9b0y1DaVPm9cB53XhcAjwgiS7q+oT4ylx0fVp8w7ge1V1B3BHki8CTwSWakD0afMr\ngU01OkC/Pcl1wOOAS8ZT4tgN+vm13A8x9bl1xxbgFd1ogOOA71fVznEXuojmbXOSI4ELgJcvk2+T\n87a5qo6qqjVVtQb4GPCaJRwO0O9/+0Lg6Un2S3IgozsjXz3mOhdTnzbfyKjHRJJVwGOBb4+1yvEa\n9PNrWfcgapZbdyT5/W793zIa0fICYDvwI0bfQJasnm1+M/ALwNndN+rdtYRvdNazzctKnzZX1dVJ\nPgN8C7gHOKeqmsMll4Kef+c/A/4hyeWMRvacUVVL9i6vST4CPAs4JMkO4C3A/WA8n19eSS1Jalru\nh5gkSXvJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU3/B3Qe7SWXY1VgAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1231cd2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of subjectivity scores\n",
    "yelp.subjectivity.plot(kind=\"hist\", bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1233c8320>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEEQiohSEARBEIqIUhAEQRCK/H8BWWg7oG3TJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1233e7748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot scatter plot of polarity vs subjectivity scores\n",
    "plt.scatter(yelp.polarity, yelp.subjectivity)\n",
    "plt.xlabel(\"Polarity Scores\")\n",
    "plt.ylabel(\"Subjectivity Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEcCAYAAAAGD4lRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1231d8358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot boxplots of the polarity by yelp stars\n",
    "yelp.boxplot(column='polarity', by='stars');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are you thoughts on the plots? Do they make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a machine learning algorithm to classify yelp reviews as either five or one stars. But first we need to transform or \"vectorize\" our raw text before make any classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer: How to turn text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-80-30bd3f055c9f>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-80-30bd3f055c9f>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print y.value_counts(normalize=True)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't pass in raw text into an algorithm, first we have to vectorize it, which means converting a collection of text documents to a matrix of token counts.\n",
    "\n",
    "<br>\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "dtm = vect.fit_transform(simple_train)\n",
    "tf = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       1        0    0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming a new sentence (based on rules set in original)\n",
    "new_sentence = ['please call yourself a taxi']\n",
    "pd.DataFrame(vect.transform(new_sentence).toarray(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? How come the two dataframes have the same features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CountVectorizer to create document-term matrices from X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Intialize vectorizer object\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Fit and transform with training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "#Transform the testing data\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "#Vectorized data shapes\n",
    "\n",
    "print X_train_dtm.shape\n",
    "print X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'00', u'000', u'00a', u'00am', u'00pm', u'01', u'02', u'03', u'03342', u'04', u'05', u'06', u'07', u'09', u'0buxoc0crqjpvkezo3bqog', u'0l', u'10', u'100', u'1000', u'1000x', u'1001', u'100th', u'101', u'102', u'105', u'1070', u'108', u'10am', u'10ish', u'10min', u'10mins', u'10minutes', u'10pm', u'10th', u'10x', u'11', u'110', u'1100', u'111', u'111th', u'112', u'115th', u'118', u'11a', u'11am', u'11p', u'11pm', u'12', u'120', u'128i']\n"
     ]
    }
   ],
   "source": [
    "# first 50 features\n",
    "print vect.get_feature_names()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'jackrabbit', u'jacks', u'jacob', u'jacques', u'jacquie', u'jacuzzi', u'jade', u'jaeger', u'jagermeister', u'jaguar', u'jaime', u'jake', u'jalapeno', u'jalapenos', u'jalape\\xf1o', u'jalepeno', u'jalpeno', u'jam', u'jamaica', u'jamaican', u'jamba', u'jambalaya', u'jamburrito', u'james', u'jameson', u'jammed', u'jamming', u'jamoca', u'jams', u'jan', u'janes', u'janet', u'janis', u'january', u'japan', u'japanese', u'japchae', u'jar', u'jared', u'jargon', u'jars', u'jasmine', u'jason', u'java', u'jaw', u'jaws', u'jay', u'jazz', u'jazzed', u'jc']\n"
     ]
    }
   ],
   "source": [
    "# Random selection of 50 features\n",
    "print vect.get_feature_names()[8000:8050]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>03342</th>\n",
       "      <th>04</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zzed</th>\n",
       "      <th>clairs</th>\n",
       "      <th>cole</th>\n",
       "      <th>m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  16825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00a  00am  00pm  01  02  03  03342  04 ...  zucchini  zuchinni  \\\n",
       "0   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "1   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "2   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "4   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "\n",
       "   zumba  zupa  zuzu  zwiebel  zzed  clairs  cole  m  \n",
       "0      0     0     0        0     0        0      0   0  \n",
       "1      0     0     0        0     0        0      0   0  \n",
       "2      0     0     0        0     0        0      0   0  \n",
       "3      0     0     0        0     0        0      0   0  \n",
       "4      0     0     0        0     0        0      0   0  \n",
       "\n",
       "[5 rows x 16825 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to configure vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** boolean, True by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a count vectorizer that doesn't lowercase the words\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape # has more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zone out', u'zone when', u'zones', u'zones dolls', u'zoning', u'zoning issues', u'zoo', u'zoo and', u'zoo is', u'zoo not', u'zoo the', u'zoo ve', u'zoyo', u'zoyo for', u'zucca', u'zucca appetizer', u'zucchini', u'zucchini and', u'zucchini bread', u'zucchini broccoli', u'zucchini carrots', u'zucchini fries', u'zucchini pieces', u'zucchini strips', u'zucchini veal', u'zucchini very', u'zucchini with', u'zuchinni', u'zuchinni again', u'zuchinni the', u'zumba', u'zumba class', u'zumba or', u'zumba yogalates', u'zupa', u'zupa flavors', u'zuzu', u'zuzu in', u'zuzu is', u'zuzu the', u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. This allows you use to use your own custom stopwords list. Great for corpus-specific stopwords, that words that aren't regular stopwords but become stopwords depending on the context.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16528)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with stop_words to english\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the stopwords used\n",
    "\n",
    "vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 2000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with max_features to 2000\n",
    "vect = CountVectorizer(max_features=2000)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4340)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with min_df to 5\n",
    "vect = CountVectorizer(min_df=5)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 128)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with min_df to 0.1\n",
    "vect = CountVectorizer(min_df=0.1)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'about',\n",
       " u'after',\n",
       " u'all',\n",
       " u'also',\n",
       " u'always',\n",
       " u'am',\n",
       " u'amazing',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'as',\n",
       " u'at',\n",
       " u'back',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'best',\n",
       " u'better',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'could',\n",
       " u'delicious',\n",
       " u'did',\n",
       " u'do',\n",
       " u'don',\n",
       " u'eat',\n",
       " u'even',\n",
       " u'ever',\n",
       " u'every',\n",
       " u'everything',\n",
       " u'experience',\n",
       " u'few',\n",
       " u'first',\n",
       " u'food',\n",
       " u'for',\n",
       " u'fresh',\n",
       " u'friendly',\n",
       " u'from',\n",
       " u'get',\n",
       " u'go',\n",
       " u'going',\n",
       " u'good',\n",
       " u'got',\n",
       " u'great',\n",
       " u'had',\n",
       " u'has',\n",
       " u'have',\n",
       " u'he',\n",
       " u'here',\n",
       " u'how',\n",
       " u'if',\n",
       " u'in',\n",
       " u'is',\n",
       " u'it',\n",
       " u'just',\n",
       " u'know',\n",
       " u'like',\n",
       " u'little',\n",
       " u'love',\n",
       " u'made',\n",
       " u'make',\n",
       " u'me',\n",
       " u'menu',\n",
       " u'more',\n",
       " u'much',\n",
       " u'my',\n",
       " u'never',\n",
       " u'nice',\n",
       " u'no',\n",
       " u'not',\n",
       " u'now',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'one',\n",
       " u'only',\n",
       " u'or',\n",
       " u'order',\n",
       " u'ordered',\n",
       " u'other',\n",
       " u'our',\n",
       " u'out',\n",
       " u'over',\n",
       " u'people',\n",
       " u'place',\n",
       " u'really',\n",
       " u'restaurant',\n",
       " u'right',\n",
       " u'say',\n",
       " u'service',\n",
       " u'so',\n",
       " u'some',\n",
       " u'staff',\n",
       " u'take',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'them',\n",
       " u'then',\n",
       " u'there',\n",
       " u'they',\n",
       " u'this',\n",
       " u'time',\n",
       " u'to',\n",
       " u'too',\n",
       " u'try',\n",
       " u'up',\n",
       " u'us',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'way',\n",
       " u'we',\n",
       " u'well',\n",
       " u'went',\n",
       " u'were',\n",
       " u'what',\n",
       " u'when',\n",
       " u'which',\n",
       " u'who',\n",
       " u'will',\n",
       " u'with',\n",
       " u'would',\n",
       " u'you',\n",
       " u'your']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What are the words that show up in at least 10 percent of documents\n",
    "\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents). Court, ball, shooting, passing will show up frequently in a basketball corpus, but essentially add no meaning. Corpus-specific stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: Ultra Violet Analytics](http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/)\n",
    "\n",
    "\"Tf-idf is a very common technique for determining roughly what each document in a set of documents is about. It cleverly accomplishes this by looking at two simple metrics: tf (term frequency) and idf (inverse document frequency). Term frequency is the proportion of occurrences of a specific term to total number of terms in a document. Inverse document frequency is the inverse of the proportion of documents that contain that word/phrase. The general idea is that if a specific phrase appears a lot of times in a given document, but it doesnt appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency with CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize vectorizer with binary = true\n",
    "vect = CountVectorizer(binary=True)\n",
    "\n",
    "#Fit and transform the text and sum up the counts\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "#Put results into dataframe\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many documents each word appears in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide tf by df\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the sklearn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize vectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "#Fit and transform using tfidf and input results into dataframe\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem covers the probabilistic relationship between multiple variables, and specifically allows us to define one conditional in terms of the underlying probabilities and the inverse condition. Specifically, it can be defined as:\n",
    "\n",
    "$$P(y|x) = P(y)P(x|y)/P(x)$$\n",
    "\n",
    "This means the probability of y given x condition equals the probability of y times the probability of x given y condition divided by the probability of x.\n",
    "\n",
    "This theorem can be extended to when x is a vector (containing the multiple x variables used as inputs for the model) to:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.**\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are **conditionally independent**, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "We can calculate all of the values in the numerator by examining a corpus of **spam email**:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of **ham email**:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the **higher probability**, and so we predict that the email is **spam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "- The **\"naive\" assumption** of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The **normalization constant** (the denominator) can be ignored since it's the same for all classes.\n",
    "- The **prior probability** is much less relevant once you have a lot of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pros</b>: \n",
    "- Very fast. Adept at handling tens of thousands of features which is why it's used for text classification\n",
    "- Works well with a small number of observations\n",
    "- Isn't negatively affected by \"noise\"\n",
    "\n",
    "<b>Cons</b>:\n",
    "- Useless for probabilities. Most of the time assigns probabilites that are close to zero or one\n",
    "- It is literally \"naive\". Meaning it assumes features are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorize the whole corpus. Remove stop words.\n",
    "\n",
    "#Intialize vectorizer\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#fit and transform data\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747919725893294"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "#Fit and score model\n",
    "\n",
    "nb.fit(X_dtm, y)\n",
    "\n",
    "nb.score(X_dtm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but let's try it on a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.819961\n",
       "1    0.180039\n",
       "Name: stars, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null accuracy of testing set\n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758485639686684\n",
      "0.9158512720156555\n"
     ]
    }
   ],
   "source": [
    "#Intialize vectorizer \n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Fit and transform on the training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#Transform the testing data witht the vectorizer\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "#Fit it on training data\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Score it on training and testing data\n",
    "print nb.score(X_train_dtm, y_train)\n",
    "print nb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you assess this model? \n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try it on some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[[0.03322787 0.96677213]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on new text\n",
    "new_text = [\"I had a decent time at this restaurant. \\\n",
    "The food was delicious but the service was very poor. \\\n",
    "I recommend the salad but do not eat the french fries.\"]\n",
    "new_text_transform = vect.transform(new_text)\n",
    "\n",
    "#Predict class\n",
    "print nb.predict(new_text_transform)\n",
    "\n",
    "#Class probabilities\n",
    "print nb.predict_proba(new_text_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again with the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204960835509139\n",
      "0.8199608610567515\n"
     ]
    }
   ],
   "source": [
    "#Intialize vectorizer \n",
    "vect = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "#Fit and transform on the training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#Transform the testing data witht the vectorizer\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "#Fit it on training data\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Score it on training and testing data\n",
    "print nb.score(X_train_dtm, y_train)\n",
    "print nb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on the results? Did you expect the scores to be lower than the Countvectorizer ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cross validate with pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573203646644666"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pipeline with tfidf vectorizer with max_features = 1000 and lowercase = true\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(max_features=1000, lowercase=True), MultinomialNB())\n",
    "\n",
    "\n",
    "#Cross validate with the pipeline and use the full raw text\n",
    "cross_val_score(pipe, X, y, cv =5, scoring = \"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search time. We could spend a whole bunch of time testing various combinations of parameters, so instead of doing that, let's use grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's import time to see how long it takes\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144.671555042\n"
     ]
    }
   ],
   "source": [
    "#Grid search for the count vectorizer\n",
    "\n",
    "grid_cv = GridSearchCV(pipe_cv, param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_cv.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__lowercase': True,\n",
       " 'countvectorizer__max_features': 10000,\n",
       " 'countvectorizer__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters\n",
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9312285854136074"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.075494051\n"
     ]
    }
   ],
   "source": [
    "#Grid search for the tfidf vectorizer\n",
    "\n",
    "grid_tf = GridSearchCV(pipe_tf, param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_tf.fit(X, y)\n",
    "#Print time elapsed\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__lowercase': True,\n",
       " 'tfidfvectorizer__max_features': 1000,\n",
       " 'tfidfvectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters\n",
    "grid_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651492902594224"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "grid_tf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Search option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.0569269657\n"
     ]
    }
   ],
   "source": [
    "#Intialize randomized grid search\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the \"spaminess\" of a token\n",
    "\n",
    "This is a really helpful technique to find the words most associated with either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                       message  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in ham or spam text dataset\n",
    "df = pd.read_table(\"../../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at null accuracy\n",
    "df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935391241923905"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign X and y\n",
    "X = df.message\n",
    "y = df.label\n",
    "\n",
    "#Intialize vectorizer with default settings\n",
    "vect =CountVectorizer()\n",
    "#Fit and transform X\n",
    "Xdtm = vect.fit_transform(X)\n",
    "#Intialize, fit, and score model on training data\n",
    "nb = MultinomialNB()\n",
    "nb.fit(Xdtm,y)\n",
    "nb.score(Xdtm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8713"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign list of features to tokens variable\n",
    "tokens = vect.get_feature_names()\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fifteen', u'fifth', u'fifty', u'fight', u'fighting', u'fightng', u'fights', u'figure', u'figures', u'figuring', u'file', u'files', u'fill', u'filled', u'filling', u'fills', u'film', u'films', u'filth', u'filthy', u'filthyguys', u'final', u'finalise', u'finally', u'finance', u'financial', u'find', u'finding', u'finds', u'fine', u'finest', u'fingers', u'finish', u'finishd', u'finished', u'finishes', u'finishing', u'fink', u'finn', u'fire', u'fired', u'firefox', u'fireplace', u'fires', u'firmware', u'firsg', u'first', u'fish', u'fishhead', u'fishrman']\n"
     ]
    }
   ],
   "source": [
    "#Print random slice of features\n",
    "print tokens[3200:3250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   1., ...,   1.,   0.,   1.],\n",
       "       [ 10.,  29.,   0., ...,   0.,   1.,   0.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many times does a word appear in each class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8713)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1., ...,  1.,  0.,  1.])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns out counts of each word in documents marked \"ham\"\n",
    "ham_token_count = nb.feature_count_[0,:]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.,  29.,   0., ...,   0.,   1.,   0.])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns out counts of each word in documents marked \"spam\"\n",
    "spam_token_count = nb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weddin</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gautham</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aproach</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algarve</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versus</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "weddin     1.0   0.0\n",
       "gautham    3.0   0.0\n",
       "lambda     1.0   0.0\n",
       "salmon     1.0   0.0\n",
       "live      17.0  29.0\n",
       "memories   1.0   0.0\n",
       "aproach    2.0   0.0\n",
       "37819      0.0   1.0\n",
       "algarve    0.0   2.0\n",
       "versus     1.0   0.0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "df_tokens = pd.DataFrame({'token':tokens, \n",
    "                          'ham':ham_token_count, \n",
    "                          'spam':spam_token_count}).set_index('token')\n",
    "\n",
    "#Randomly data \n",
    "df_tokens.sample(10, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weddin</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gautham</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aproach</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algarve</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versus</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "weddin     2.0   1.0\n",
       "gautham    4.0   1.0\n",
       "lambda     2.0   1.0\n",
       "salmon     2.0   1.0\n",
       "live      18.0  30.0\n",
       "memories   2.0   1.0\n",
       "aproach    3.0   1.0\n",
       "37819      1.0   2.0\n",
       "algarve    1.0   3.0\n",
       "versus     2.0   1.0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "df_tokens['ham'] = df_tokens.ham + 1\n",
    "df_tokens['spam'] = df_tokens.spam + 1\n",
    "df_tokens.sample(10, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4825.,   747.])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weddin</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gautham</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.040161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aproach</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.002677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algarve</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.004016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versus</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham      spam\n",
       "token                       \n",
       "weddin    0.000415  0.001339\n",
       "gautham   0.000829  0.001339\n",
       "lambda    0.000415  0.001339\n",
       "salmon    0.000415  0.001339\n",
       "live      0.003731  0.040161\n",
       "memories  0.000415  0.001339\n",
       "aproach   0.000622  0.001339\n",
       "37819     0.000207  0.002677\n",
       "algarve   0.000207  0.004016\n",
       "versus    0.000415  0.001339"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "df_tokens['ham'] = df_tokens.ham / nb.class_count_[0]\n",
    "df_tokens['spam'] = df_tokens.spam / nb.class_count_[1]\n",
    "df_tokens.sample(10, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weddin</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>3.229585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gautham</th>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>1.614793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>3.229585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>3.229585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>10.765283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>3.229585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aproach</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>2.153057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>12.918340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algarve</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>19.377510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versus</th>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>3.229585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham      spam  spam_ratio\n",
       "token                                   \n",
       "weddin    0.000415  0.001339    3.229585\n",
       "gautham   0.000829  0.001339    1.614793\n",
       "lambda    0.000415  0.001339    3.229585\n",
       "salmon    0.000415  0.001339    3.229585\n",
       "live      0.003731  0.040161   10.765283\n",
       "memories  0.000415  0.001339    3.229585\n",
       "aproach   0.000622  0.001339    2.153057\n",
       "37819     0.000207  0.002677   12.918340\n",
       "algarve   0.000207  0.004016   19.377510\n",
       "versus    0.000415  0.001339    3.229585"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the ratio of spam-to-ham for each token\n",
    "df_tokens['spam_ratio'] = df_tokens.spam / df_tokens.ham\n",
    "df_tokens.sample(10, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>736.345382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.125837</td>\n",
       "      <td>607.161981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>465.060241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.081660</td>\n",
       "      <td>394.009371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.069612</td>\n",
       "      <td>335.876841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.068273</td>\n",
       "      <td>329.417671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>290.662651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>290.662651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.056225</td>\n",
       "      <td>271.285141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awarded</th>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.052209</td>\n",
       "      <td>251.907631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spam_ratio\n",
       "token                                     \n",
       "claim       0.000207  0.152610  736.345382\n",
       "prize       0.000207  0.125837  607.161981\n",
       "150p        0.000207  0.096386  465.060241\n",
       "tone        0.000207  0.081660  394.009371\n",
       "18          0.000207  0.069612  335.876841\n",
       "guaranteed  0.000207  0.068273  329.417671\n",
       "500         0.000207  0.060241  290.662651\n",
       "cs          0.000207  0.060241  290.662651\n",
       "1000        0.000207  0.056225  271.285141\n",
       "awarded     0.000207  0.052209  251.907631"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "df_tokens.sort_values('spam_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, the top ten \"spammiest\" words in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Tokenization:\n",
    "- http://text-processing.com/demo/tokenize/\n",
    "- https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "\n",
    "\n",
    "POS tagging:\n",
    "- https://nlp.stanford.edu/software/tagger.shtml\n",
    "- http://language.worldofcomputing.net/pos-tagging/parts-of-speech-tagging.html\n",
    "- https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "NLTK:\n",
    "- https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "- http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "\n",
    "TextBlob:\n",
    "- http://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "- http://rwet.decontextualize.com/book/textblob/\n",
    "- http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html\n",
    "\n",
    "Stemming and Lemmatization:\n",
    "- http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "- https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "Vectorizating Text:\n",
    "- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "- http://planspace.org/20150524-tfidf_is_about_what_matters/\n",
    "- http://www.tfidf.com/\n",
    "- http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "\n",
    "Text classification:\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "- https://www.dataquest.io/blog/natural-language-processing-with-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab time\n",
    "- There are three other datasets pitchfork album reviews, fake/real news, and political lean.\n",
    "- Pick one of those three datasets and try to build a model that differentiate between good/bad review, real/fake news, or liberal/conservative leaning. Make sure to examine the false positives and the false negatives texts. Use the \"spamminess\" technique on the corpus as well. \n",
    "- Use both count and tfidf vectorizers. Use textblob to determine sentiment and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
