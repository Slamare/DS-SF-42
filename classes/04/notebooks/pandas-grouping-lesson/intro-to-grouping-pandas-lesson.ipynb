{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Intermediate Pandas\n",
    "\n",
    "-  Map expressions\n",
    "- Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "*After this lesson, you will be able to:*\n",
    "- Get comfortable with the **.map()** method on DataFrames for batch manipulations.\n",
    "- Know what situations **grouping** is useful for\n",
    "- Explain and use the **`.groupby()`** function in Pandas\n",
    "- Demonstrate aggregation and plotting methods by groups in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup / Review / Intro to \"map\" expressions\n",
    "\n",
    "Say we have some data on median transactions per month for our users..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_list = [\n",
    "    \"5\",\n",
    "    \"12\",\n",
    "    \"0\",\n",
    "    \"9\",\n",
    "    \"45\",\n",
    "    \"6\",\n",
    "    \"39\",\n",
    "    \"84\",\n",
    "    \"40\",\n",
    "    \"19\",\n",
    "    \"23\",\n",
    "    \"87\",\n",
    "    \"26\",\n",
    "    \"1\"   \n",
    "]\n",
    "#this is in strings so we need to adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5374f219967d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransaction_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    ".head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the mean of these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(transaction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that didn't work.\n",
    "\n",
    "### <font color=blue>(Independent)</font> Activity: Process the `transaction_data` so its mean can be properly taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_list_int = []\n",
    "\n",
    "for transaction in transaction_list:\n",
    "    transaction_list_int.append(int(transaction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# type(transaction_list_int[0])\n",
    "np.mean(transaction_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete activity below\n",
    "# new_transacation = pd.to_numeric(transaction_list)\n",
    "np.median(transaction_list_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if our data was in a `DataFrame` or a `Series` instead of a `list`. It is possible to loop through pandas objects, just like we did with the list, but that isn't a best practice. Instead pandas offers the `.map()` method, dedicated functionality to perform iterative-like transformations.\n",
    "\n",
    "**Pandas type transformation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create DataFrame\n",
    "#to transform these strings we use map method\n",
    "transaction_df = pd.DataFrame(transaction_list, columns = [\"median_transaction_count\"])\n",
    "transaction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(codealong) Convert the transaction_count column into Ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To Do\n",
    "transaction_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_df[\"median_transaction_count\"] = transaction_df[\"median_transaction_count\"].map(int)\n",
    "\n",
    "#map takes a function, element by element by each value in the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** `.map()` is a `Series` method, it will not work on `DataFrames`. Instead you need either `.applymap()` or `.apply()`, which will come up later on.\n",
    "\n",
    "#### How do we handle converting types if we have messier data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transaction_messy_list = [\n",
    "    \"5\",\n",
    "    \"12\",\n",
    "    \"0\",\n",
    "    \"9\",\n",
    "    \"?\",\n",
    "    \"6\",\n",
    "    \"missing\",\n",
    "    \"84\",\n",
    "    \"40\",\n",
    "    \"?\",\n",
    "    \"23\",\n",
    "    \"87\",\n",
    "    \"26\",\n",
    "    \"one\",\n",
    "    \"7\"\n",
    "]\n",
    "\n",
    "#Create df\n",
    "transaction_messy_df = pd.DataFrame(transaction_messy_list, columns = [\"median_transaction_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"map\" custom functions that we right ourselves. This gives us ALOT of flexibility.\n",
    "\n",
    "### <font color=blue>(Independent)</font> Activity: Write a function to process a single transaction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_transaction_value(number_string):\n",
    "    \"\"\"\n",
    "    Convert strings of numeric values to Int, otherwise return np.nan\n",
    "    \"\"\"\n",
    "    \n",
    "    #Your work here\n",
    "    for value in number_string:\n",
    "        if (type(number_string) == str) :\n",
    "            str_df[\"covert string to int\"].map(int)\n",
    "        else :\n",
    "            number_string.isnull()\n",
    "            \n",
    "    print clean_transaction_value\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_transaction_value(number_string):\n",
    "    \"\"\"\n",
    "    Convert strings of numeric values to Int, otherwise return np.nan\n",
    "    \"\"\"\n",
    "    \n",
    "    #Your work here\n",
    "    try: \n",
    "        return int(number_string) #thing you want to do\n",
    "    except :\n",
    "        return np.nan  #thing to do in case of error\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_string = [\"4\", \"7\", \"3\"]\n",
    "\n",
    "def clean_transaction_value(number_string):\n",
    "    \"\"\"\n",
    "    Convert strings of numeric values to Int, otherwise return np.nan\n",
    "    \"\"\"\n",
    "    \n",
    "    #Your work here\n",
    "    for value in number_string:\n",
    "        if number_string.isdigit() :\n",
    "            return int(number_string)\n",
    "        else: \n",
    "            print np.nan\n",
    "\n",
    "\n",
    "# print clean_transaction_value(number_string)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this function to clean our `transaction_messy_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (codealong) to do\n",
    "\n",
    "transaction_messy_df[\"median_transaction_count\"].map(clean_transaction_value)\n",
    "\n",
    "transaction_messy_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction to Pandas Grouping\n",
    "\n",
    "---\n",
    "\n",
    "### Lesson Guide\n",
    "\n",
    "- [Overview of multi-dimensional data analysis](#overview)\n",
    "- [Examples of when to group data](#grouping_examples)\n",
    "- [Load the Titanic dataset](#load_titanic)\n",
    "- [Introduction to pandas `.groupby()`](#groupby)\n",
    "- [Grouping by multiple variables](#groupby_multiple)\n",
    "- [Applying basic functions to groups](#basic_functions)\n",
    "- [Removing the hierarchical index](#removing_hierarchical)\n",
    "- [Applying custom functions with apply](#custom_functions)\n",
    "- [Plotting basic histograms with groups](#basic_plotting)\n",
    "- [Grouped histograms](#grouped_hists)\n",
    "- [Independent practice](#independent_practice)\n",
    "\n",
    "\n",
    "\n",
    "> ## The goal of grouping and aggregation is to describe segments of your data based on unique values of a feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grouping_examples'></a>\n",
    "\n",
    "### Analyzing data by group: some specific examples\n",
    "\n",
    "---\n",
    "\n",
    " - Sum of crimes by time of day in SF (morning, afternoon, night)\n",
    " - Count number of people with the same last name\n",
    " - Median number of multi-unit buildings in a region\n",
    " - Popularity of movie genres by region\n",
    " - Segmenting customers based on age, buying habbits, interests, behavior\n",
    " - Using the \"GROUP BY\" clause in a database query using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Example\n",
    "\n",
    "---\n",
    "\n",
    "### Count of crimes by day\n",
    "\n",
    "index| day_of_week| time_of_day | suspect_apprehended \n",
    "- |------|------|------ \n",
    "0 |   tuesday | night| 1\n",
    "1 |   friday | afternoon| 1\n",
    "2 |   thursday  | afternoon| 0\n",
    "3 |   friday  | night| 1\n",
    "4 |   friday | night| 0\n",
    "5 |   tuesday | night| 0\n",
    "6 |   thursday | morning| 1\n",
    "7 |   friday | night| 1\n",
    "\n",
    "> *Which day of the week has the most crime?*\n",
    "\n",
    "In SQL you use a `GROUP BY` statement like:\n",
    "\n",
    "```SQL\n",
    "SELECT day_of_week,\n",
    "       COUNT(*)\n",
    "FROM crimes;\n",
    "```\n",
    "\n",
    "Pandas has its own grouping and aggregation syntax:\n",
    "```python\n",
    "crime_df.groupby(\"day_of_week\").size()\n",
    "```\n",
    "\n",
    "Either of these operations would produce table something like:\n",
    "\n",
    "index| day_of_week| count\n",
    "- |------|------|------ \n",
    "0 |   tuesday | 2\n",
    "1 |   thursday | 2\n",
    "2 |   friday  | 4\n",
    "\n",
    "You can see that the resulting dataframe has **unique** values for `day_of_week` because that's the column we grouped by. The `.size()` aggregator method returns how many rows are in each group.\n",
    "\n",
    "\n",
    "### Grouping by multiple columns\n",
    "\n",
    "> *Which `day_of_week` / `time_of_day` has the highest apprehension rate?*\n",
    "\n",
    "Now we want to aggegrate on all unique combinations of both of those columns.\n",
    "\n",
    "In SQL:\n",
    "\n",
    "```SQL\n",
    "SELECT day_of_week,\n",
    "       time_of_day,\n",
    "       AVG(suspect_apprehended) AS apprehension_rate\n",
    "FROM crimes\n",
    "GROUP BY day_of_week,\n",
    "         time_of_day;\n",
    "```\n",
    "\n",
    "In Pandas:\n",
    "\n",
    "```python\n",
    "crime_df.groupby([\"day_of_week\", \"time_of_day\"])[\"apprehended\"].mean()\n",
    "```\n",
    "\n",
    "We get:\n",
    "\n",
    "index| day_of_week| time_of_day | apprehension_rate\n",
    "- |------|------|------ \n",
    "0 |   tuesday | night | 0.5 \n",
    "1 |   thursday | morning | 1\n",
    "2 |   thursday | afternoon | 0\n",
    "3 |   friday  | afternoon | 1\n",
    "4 |   friday  | night | 0.66\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset aggregation:\n",
    "\n",
    "This chart stratifies a single variable \"Industry\", **counting** job openings within category.\n",
    "\n",
    "![](http://www.rasmussen.edu/images/blogs/1360270834-402_Graphs_JobOpeningsByIndustry.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical aggregation\n",
    "\n",
    "This chart aggregates first by a top level group, \"industry\", and then a secondary group \"date\" within each industry.\n",
    "\n",
    "![](http://junkcharts.typepad.com/.a/6a00d8341e992c53ef0192acc65090970d-pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Anatomy of the `groupby` statement\n",
    "\n",
    "---\n",
    "\n",
    "![](https://i.imgur.com/ZvFed5Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_titanic'></a>\n",
    "\n",
    "### Exploring the Titanic dataset with grouping\n",
    "\n",
    "---\n",
    "\n",
    "To explore the power of grouping with pandas we will be using [the famous Titanic dataset that can be downloaded from Kaggle](https://www.kaggle.com/c/titanic). From the competition decription:\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "Though we will not be doing any modeling of survival rates in this lesson, there are interesting patterns to be found just by exploring descriptive statistics in cross-sections of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data into pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File titanic_clean.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-98df4da99c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_to_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./datasets/titanic_clean.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic_clean.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File titanic_clean.csv does not exist"
     ]
    }
   ],
   "source": [
    "path_to_file = './datasets/titanic_clean.csv'\n",
    "pd.read_csv('titanic_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains a variety of information about passengers involved in the sinking of the Titanic.\n",
    "\n",
    "**Describe the data in the columns with summary statistics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='groupby'></a>\n",
    "\n",
    "### Intro to `.groupby()` in pandas\n",
    "\n",
    "---\n",
    "\n",
    "The built-in `.groupby()` functions for dataframes is one of the most useful tools in pandas. As the name implies, `.groupby` will group your dataset by one or more user-specified column names.\n",
    "\n",
    "**Using `.groupby`, create a grouped dataframe where the titanic dataset is grouped by \"Pclass\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(\"Pclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the type of the grouped dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for group in grouped:\n",
    "    print \"group_type\", group[0]\n",
    "    print \"head\", group[1].head()\n",
    "    print \"--------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a `DataFrame` object we now have a `DataFrameGroupBy` object. This operates somewhat differently than the DataFrame we are used to, as we shall see.\n",
    "\n",
    "**Try pulling out the first group from the grouped dataframe with index 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped dataframes do not work the same as python lists. You can't pull out the different groups with indexers. Despite this, grouped dataframe objects **are** iterable! You can step through them using a for-loop, for example.\n",
    "\n",
    "In our grouped dataframe, each element will be a tuple containing the Pclass group as its first element, and the subset of the original titanic dataframe for that Pclass as it's second element.\n",
    "\n",
    "**Write a for-loop to iterate through the grouped dataframe, printing out the PClass and the header of the subset each time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='groupby_multiple'></a>\n",
    "\n",
    "### Grouping by multiple fields\n",
    "\n",
    "---\n",
    "\n",
    "Grouping by more than one column is simple. The `.groupby()` function can take a list of column names to group by. When you group by more than one column, each subset of the data will correspond to one of the distinct combinations of the grouping columns.\n",
    "\n",
    "**Create a grouped dataframe by grouping the titanic data by \"Pclass\" and \"Survived\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(grouped)\n",
    "len(titanic.groupby([\"Pclass\", \"Survived\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the length of this grouped dataframe.**\n",
    "\n",
    "It is the same length as unique combinations of Pclass and Survied: 3 Pclasses by 2 Survival values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_functions'></a>\n",
    "\n",
    "### Applying basic functions to groups\n",
    "\n",
    "---\n",
    "\n",
    "Pandas makes it easy to apply basic statistical functions to the grouped data with built-in functions. For example, if you have a grouped dataframe `grouped`:\n",
    "\n",
    "```python\n",
    "print group.mean()\n",
    "print group.median()\n",
    "print group.count()\n",
    "print group.max()\n",
    "```\n",
    "\n",
    "We can get the mean, median, count, and max of the columns by group. \n",
    "\n",
    "**Try out these built in functions on the grouped data you made above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(\"Embarked\")[\"Survived\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also operate on single columns or subsets of columns across grouped dataframes using the indexing syntax for standard dataframes.\n",
    "\n",
    "**Find the percent of passengers who survived by where they embarked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_class_group = titanic.groupby([\"Embarked\", \"Pclass\"])[\"Fare\", \"Age\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the average fare and age grouped by the location embarked and the class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_class_group = emb_class_group.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='removing_hierarchical'></a>\n",
    "\n",
    "### Removing the hierarchical index\n",
    "\n",
    "---\n",
    "\n",
    "By default pandas will give you back your groups in a hierarchical index format. If this is not preferable to you, you can use the `.reset_index()` function to take your row labels and convert them into columns.\n",
    "\n",
    "**Remove the hierarchical index for the average fare and age dataset you just created, converting Embarked and Pclass to columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom_functions'></a>\n",
    "\n",
    "### Applying your own functions to groups with `.apply()`\n",
    "\n",
    "---\n",
    "\n",
    "While pandas does contain a variety of useful built-in summary functions, it is common that you will need to apply a custom function to the data in your groups. \n",
    "\n",
    "The `.apply()` function takes a function as an argument and applies it to the subsets of data in your dataframe groups.\n",
    "\n",
    "**See what happens when you replace the built-in `.mean()` function with a `.apply(np.mean)` for the question above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(\"Embarked\")[\"Pclass].apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to have the mean of fare and age per embarked and pclass, but we wanted the numbers to be rounded. One way to do this would be to round the columns after we had applied the mean function as we did above. \n",
    "\n",
    "Another way would be to write a custom function to pass into apply. *The function passed to `.apply()` will be run on all of the subsets of data.*\n",
    "\n",
    "**<font color=blue>(Independent)</font> Write a function that will take the mean of a pandas Series and round the value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply your custom function to the grouped data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='basic_plotting'></a>\n",
    "\n",
    "### Basic pandas histograms with grouped data\n",
    "\n",
    "---\n",
    "\n",
    "We can leverage the power of pandas even more by mixing its plotting capabilities with its grouping capabilities.\n",
    "\n",
    "**First find the number of passengers per PClass by using `.groupby` and `.size`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a series object with the counts of passengers per class group. It is simple to get a histogram of these counts by appending `.plot(kind=\"bar\", color=\"g\", width=0.85)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>(Independent)</font> Plot the average fare per sex and class as a barplot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grouped_hists'></a>\n",
    "\n",
    "### Grouped histograms with pandas\n",
    "\n",
    "---\n",
    "\n",
    "In the chart we just made, each bar represents a distinct combination of our groups in the `.groupby`. This is fine, but it would be a more visually appealing and informative chart if we had one of the groups as different colors and could make a grouped bar chart.\n",
    "\n",
    "**Calculate the mean of Fare by Pclass and Sex using `groupby`, assign it to a variable, and print it out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another built-in function for pandas objects called `.unstack()`. When we have a hierarchical index like we do above with Pclass as the broader category and Sex as the sub-category, the `.unstack()` command will attempt to move the subcategory from an index to column representation.\n",
    "\n",
    "This is a way to move from a \"long\" to \"wide\" column format.\n",
    "\n",
    "**Use the `.unstack()` function on your mean fare variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use the plot function on the unstacked data to create a bar chart.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you add the keyword variable `stacked=True` it will instead stack the bars within the broader Pclass category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
