{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Classification with K-Nearest Neighbors\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Alexander Barriga (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the difference between classification and regression models\n",
    "- Understand the K-Nearest Neighbors algorithm visually and in pseudocode\n",
    "- Explain the differences between distance metrics and explore the two most common\n",
    "- Apply KNN classification to the Wisconsin breast cancer dataset\n",
    "- Practice manually performing stratified cross-validation\n",
    "- Visually examine the effect of K neighbors on the decision boundary\n",
    "- Explain the effect of choosing K on the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: regression vs. classification\n",
    "\n",
    "We've discussed the difference between continuous and discrete numbers. That difference is what distinguishes classification from regression prediction tasks. Today we are going to focus on predicting non-quantitative, discrete categories, which is known as classification.\n",
    "\n",
    "Take wine for example. You could predict a quality rating using regression, but what if we just wanted to predict whether wine was good or bad? Red or white? \n",
    "\n",
    "Classification algorithms do just that; they predict categories, or classes. Split the data into groups and place new data into those groups. \n",
    "\n",
    "![](http://ipython-books.github.io/images/ml.png \"Best Split vs Best Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-visual-intro'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN) visually\n",
    "\n",
    "**KNN works similarly to how we humans might choose to classify things. Below we have some red and blue dots:**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_reds_and_blues.png \"Some Dots\")\n",
    "\n",
    "**A new dot appears without a color and we need to decide which color it is most likely going to be.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point.png \"A New Dot Appears\")\n",
    "\n",
    "**We compare it to its three nearest neighbors â€“ its neighbors are more often red, so we label it red.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred.png \"3 Nearest Neighbors\")\n",
    "\n",
    "**What if we increase the number of neighbors to consider to 5?**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred_blue.png \"5 Nearest Neighbors\")\n",
    "\n",
    "**This is in essence the K-Nearest Neighbors (KNN) algorithm. The K represents the number of \"neighbors\" you use.**\n",
    "\n",
    "> ***Images above credited to the yhat blog.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## The KNN algorithm\n",
    "\n",
    "---\n",
    "\n",
    "K-Nearest Neighbors takes a unique approach to finding patterns in the data. In order to estimate a value (regression) or class membership (classification), the algorithm finds the observations in its training data that are \"nearest\" to the observation to predict. It then takes a vote of those training observations' target values to estimate the value for the new data point.\n",
    "\n",
    "Distance is usually calculated using the euclidean distance. The \"K\" in KNN refers to the number of nearest neighbors that will be contributing to the prediction. \n",
    "\n",
    "Today we will be looking at KNN only in the context of classification.\n",
    "\n",
    "**The KNN can be concisely represented with pseudocode:**\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "> **Note**: in the case of ties, sklearn's `KNeighborsClassifier()` will just choose the first class (when weights are uniform)! If this is unappealing to you you can change the weights keyword argument to 'distance'. More on this later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance'></a>\n",
    "## The KNN distance metric\n",
    "\n",
    "---\n",
    "KNN typically uses one of two distance metrics: euclidean or manhattan. Other distance metrics are possible, but more rare (sometimes it makes sense to create your own distance function.\n",
    "\n",
    "<a id='euclidean'></a>\n",
    "### Euclidean distance\n",
    "\n",
    "Recall the famous Pythagorean Theorem\n",
    "![Alt text](http://ncalculators.com/images/pythagoras-theorem.gif)\n",
    "\n",
    "We can apply the theorem to calculate distance between points. This is called Euclidean distance. \n",
    "\n",
    "![Alt text](http://rosalind.info/media/Euclidean_distance.png)\n",
    "\n",
    "### $$\\text{Euclidean  distance}=\\sqrt{(x_1-x_2)^2+(y_1-y_1)^2}$$\n",
    "\n",
    "There are many different distance metrics, but Euclidean is the most common (and default in sklearn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wisconsin'></a>\n",
    "\n",
    "## Load the wine dataset\n",
    "\n",
    "---\n",
    "\n",
    "Below we will be testing out the KNN classification algorithm on the classic [UCI Wine Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are looking to classify wine \n",
    "wine_loader = load_wine()\n",
    "\n",
    "wine_loader\n",
    "#not as nice as a CSV, which we would typically open\n",
    "#we are going to discuss how to troubleshoot \n",
    "#for this there is a {} brackets so it might be a dictonary, there is also a Key 'DESCR'\n",
    "#and the colon \n",
    "type(wine_loader)\n",
    "#its inherited from a dicetionary \n",
    "isinstance(wine_loader, dict)\n",
    "#tells us if it is a dictionary OR built on top of a dictionary\n",
    "#what does built on top of a dictionary mean??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_names\n",
      "data\n",
      "target\n",
      "DESCR\n",
      "feature_names\n"
     ]
    }
   ],
   "source": [
    "for key, val in wine_loader.items():\n",
    "    print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Data Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- 1) Alcohol\n",
      " \t\t- 2) Malic acid\n",
      " \t\t- 3) Ash\n",
      "\t\t- 4) Alcalinity of ash  \n",
      " \t\t- 5) Magnesium\n",
      "\t\t- 6) Total phenols\n",
      " \t\t- 7) Flavanoids\n",
      " \t\t- 8) Nonflavanoid phenols\n",
      " \t\t- 9) Proanthocyanins\n",
      "\t\t- 10)Color intensity\n",
      " \t\t- 11)Hue\n",
      " \t\t- 12)OD280/OD315 of diluted wines\n",
      " \t\t- 13)Proline\n",
      "        \t- class:\n",
      "                - class_0\n",
      "                - class_1\n",
      "                - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      "References\n",
      "----------\n",
      "(1) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "Comparison of Classifiers in High Dimensional Settings, \n",
      "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Technometrics). \n",
      "\n",
      "The data was used with many others for comparing various \n",
      "classifiers. The classes are separable, though only RDA \n",
      "has achieved 100% correct classification. \n",
      "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "(All results using the leave-one-out technique) \n",
      "\n",
      "(2) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Journal of Chemometrics). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print wine_loader[\"DESCR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <font color=blue>Independent - </font>Create a DataFrame from this strange `wine` object. Don't forget to add column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "# DataFrame.from_dict\n",
    "# DataFrame.wine_loader\n",
    "type(wine_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data1 = pd.DataFrame(data= np.c_[iris['data'],\n",
    "df = pd.DataFrame([wine_loader])\n",
    "df.head()\n",
    "df.columns\n",
    "type(df)\n",
    "#changed the wine_loader into a dataframe to be able to better \n",
    "#manipulate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gus's way\n",
    "wine_loader.data\n",
    "#this is a numpy array and you can use this, but it wil make things\n",
    "#difficult down the line because there aren't any column names\n",
    "y = wine_loader.target\n",
    "wine_df = pd.DataFrame(wine_loader.data, columns=wine_loader.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlations'></a>\n",
    "## Examine the correlation structure of the dataset\n",
    "\n",
    "---\n",
    "\n",
    "You should exclude the `id` column as this is just an indicator variable for the subject.\n",
    "\n",
    "<a id='heatmap'></a>\n",
    "### Method 1: plot a heatmap of the correlation matrix\n",
    "\n",
    "Plot a seaborn heatmap of the correlation matrix to visually examine which variables are correlated and anti-correlated, and to what degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11101bb10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wine_df.corr()\n",
    "sns.heatmap(wine_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x10d66e350>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "sns.pairplot(wine_df.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pairplot'></a>\n",
    "### Method 2: Use seaborn's pairplot to visualize relationships between variables\n",
    "\n",
    "When you have a small number of predictor variables, seaborn's `pairplot` function will give you a more detailed visual look at the relationships between variables. The pairplot is similar to a correlation matrix, but displays scatterplots of variable pairs. Along the diagonal line are histograms showing the distribution of each variable.\n",
    "\n",
    "One of the most appealing aspects of the pairplot function for classification tasks is that the scatterplots and histograms can be split along a hue variable. If we use the `malignant` target class as the hue we are able to see how the classes are distributed across these variables as well.\n",
    "\n",
    "Plot data using seaborn's `pairplot()` function. The hue will be the class variable \"malignant\". The variables will be the other columns excluding, of course, the subject ID column. This function can take some time to run.\n",
    "\n",
    "> **Note:** Most of these predictors are highly correlated with the \"class\" variable. This is already an indication that our classifier is very likely to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prediction: [2 2 2 1 0]\n",
      "actual:  [2 2 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# A:\n",
    "\n",
    "#1. import \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#2. instantiate ---- the paranthesis is critical to differntiating\n",
    "# so that it instantiate\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "#5 is ?????\n",
    "\n",
    "#3. fit (aka \"train\" on your labeled data)\n",
    "\n",
    "knn.fit(wine_df, y)\n",
    "#a method is a function that requires parathesis (fit is a method)\n",
    "\n",
    "knn.predict(wine_df.sample(3))\n",
    "#gives array with the class type predictions\n",
    "#the numerical lables \n",
    "\n",
    "# 4.Make a new prediction or score\n",
    "wine_sample = wine_df.sample(5)\n",
    "\n",
    "print \"prediction:\", knn.predict(wine_sample)\n",
    "print \"actual: \", y[wine_sample.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kneighborsclassifier'></a>\n",
    "\n",
    "## Using sklearn's `KNeighborsClassifier`\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how the sklearn KNN classifier performs on our dataset predicting the malignant target class using cross-validation.\n",
    "\n",
    "Load the KNN classifier like so:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "**We are going to set some arguments when instantiating the model:**\n",
    "1. **n_neighbors** specifies how many neighbors will vote on the class\n",
    "2. **weights** uniform weights indicate that all neighbors have the same weight\n",
    "3. **metric** and **p**: when distance is minkowski (the default) and p == 2 (the default), _this is equivalent to the euclidean distance metric_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-predictors'></a>\n",
    "### Create your target vector and predictor matrix\n",
    "\n",
    "The target should be the binary `malignant` column. The predictors are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: See above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit our first K-nearest neighbors model to the wine data\n",
    "\n",
    "The steps to using an sklearn model are:\n",
    "1. Instantiated the model\n",
    "2. Fit the instantiated model object to day (`X` and `y`)\n",
    "3. Score or make predictions with your \"trained model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "### Calculate the \"baseline\" accuracy\n",
    "\n",
    "Before we can evaluate whether our classifier's accuracy is good or bad, we need to know the baseline accuracy.\n",
    "\n",
    "**The baseline accuracy is the proportion of the majority class.**\n",
    "\n",
    "For a binary classification, this means that the baseline accuracy is the percent of the dataset that is labeled malignant or benign, depending on whichever of malignant or benign is greater. This can be calculated:\n",
    "\n",
    "```python\n",
    "baseline = np.mean(y)  # if np.mean(y) is >= 0.5\n",
    "baseline = 1. - np.mean(y) # if np.mean(y) is < 0.5\n",
    "```\n",
    "\n",
    "**It is critical that you know your baseline accuracy!**\n",
    "\n",
    "If your dataset for example had 95 1's and 5 0's, and you got a 95% accuracy using KNN, if you had not looked at your baseline accuracy you may conclude that your classifier is doing great. In fact, it's doing no better than chance! The classifier could have guessed only 1's and gotten a 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.398876\n",
       "0    0.331461\n",
       "2    0.269663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "pd.Series(y).value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn5'></a>\n",
    "### What is the accuracy for a KNN model with 5 neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7865168539325843"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "y_hat = knn.predict(wine_df)\n",
    "(y_hat == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7865168539325843"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#equivalent to above\n",
    "knn.score(wine_df,y)\n",
    "\n",
    "#when considering a model two key factors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn1'></a>\n",
    "## Tuning your model for performance\n",
    "\n",
    "As you can see the accuracy is already very high with 5 neighbors on the full dataset, but in industry, mere percentage point gains in performance could be a matter of millions of dollars. Let's see how well we can do!\n",
    "\n",
    "Right now we have two main dials to turn:\n",
    "    1. Feature selection ie which column or columns to include in the training set\n",
    "    2. Choice of `n_neighbors` aka `k`\n",
    "\n",
    "### <font color=blue>Partner Work (25 minutes)</font>   - Feature selection\n",
    "We are going to start on feature selection. With your partner, start by exploring which columns will produce the best accuracy score using the default `n_neighbors=5`.\n",
    "\n",
    "After our exploration, we will convene to share our findings as a class.\n",
    "\n",
    "#### Teams\n",
    "\n",
    "<img src=https://i.imgur.com/HFFCeUH.png width=\"30%\" align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature selection strategies:\n",
    "# -segmenting based on the different class labels (is the mean different but also considering\n",
    "#                                                 variance)\n",
    "#     - group bys\n",
    "#     - overlaid histograms\n",
    "-trial and error \n",
    "    - check features (comparing/combined columns)\n",
    "    - brute force\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [alcohol, malic_acid, ash, alcalinity_of_ash, ...\n",
       "Name: feature_names, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [2 0 1 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 2 0 2 2 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0\n",
      " 2 1 0 1 1 1 1 1 1 0 2 1 2 1 1 0 1 2 0 1 0 1 2 0 0 1 0 1 2 1 0 2 2 1 1 1 2\n",
      " 0 1 1 2 1 1 1 1 2 0 0 1 2 0 1 0 0 1 2 1 2 0 1 1 1 2]\n",
      "actual:  [2 0 1 1 1 1 2 2 1 1 2 2 2 2 1 1 2 1 1 0 0 0 2 1 0 0 2 2 0 1 0 2 1 1 0 1 0\n",
      " 1 2 0 2 1 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 2 2 0 0 1 0 1 0 2 0 1 2 1 2 2 2\n",
      " 0 2 1 0 1 1 2 1 1 0 0 1 1 0 1 0 1 1 0 1 2 0 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#1. import \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#2. instantiate \n",
    "knn = KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "#3. fit (aka \"train\" on your labeled data)\n",
    "knn.fit(wine_df, y)\n",
    "knn.predict(wine_df.sample(100))\n",
    "\n",
    "# 4.Make a new prediction or score\n",
    "wine_sample = wine_df.sample(100)\n",
    "\n",
    "print \"prediction:\", knn.predict(wine_sample)\n",
    "print \"actual: \", y[wine_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.398876\n",
       "0    0.331461\n",
       "2    0.269663\n",
       "dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "pd.Series(y).value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966292134831461"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = knn.predict(wine_df)\n",
    "(y_hat == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Partner Work (25 minutes)</font>   - Hyper-parameter tuning\n",
    "\n",
    "Using the best features, explore what the best value of `k` is for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 0.7247191011235955)\n",
      "(28, 0.7247191011235955)\n",
      "(27, 0.7247191011235955)\n",
      "(26, 0.7247191011235955)\n",
      "(25, 0.7303370786516854)\n",
      "(24, 0.7247191011235955)\n",
      "(23, 0.7247191011235955)\n",
      "(22, 0.7247191011235955)\n",
      "(21, 0.7247191011235955)\n",
      "(20, 0.7191011235955056)\n",
      "(19, 0.7247191011235955)\n",
      "(18, 0.7247191011235955)\n",
      "(17, 0.7247191011235955)\n",
      "(16, 0.7303370786516854)\n",
      "(15, 0.7191011235955056)\n",
      "(14, 0.7359550561797753)\n",
      "(13, 0.7584269662921348)\n",
      "(12, 0.7752808988764045)\n",
      "(11, 0.7696629213483146)\n",
      "(10, 0.7921348314606742)\n",
      "(9, 0.7752808988764045)\n",
      "(8, 0.7752808988764045)\n",
      "(7, 0.7471910112359551)\n",
      "(6, 0.7752808988764045)\n",
      "(5, 0.7865168539325843)\n",
      "(4, 0.8258426966292135)\n",
      "(3, 0.8707865168539326)\n",
      "(2, 0.8764044943820225)\n",
      "(1, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# A: \n",
    "\n",
    "for k in range(1,30)[::-1]:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(wine_df,y)\n",
    "    print(k, knn.score(wine_df,y))\n",
    "    \n",
    "#using the training set we got 100% b/c the points are being scored agansit itself \n",
    "#the points that weren't used in training are key to test out \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction to training and test sets\n",
    " \n",
    "Our model has already seen and fit on the train data that we are using to produce an accuracy score.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*-8_kogvwmL1H6ooN1A1tsQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test that simulates fresh data that model might be predicting on when it is put into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_df, y)  #order matters!!!\n",
    "#when k is low it is simple to outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 0.64444444444444449)\n",
      "(28, 0.66666666666666663)\n",
      "(27, 0.64444444444444449)\n",
      "(26, 0.64444444444444449)\n",
      "(25, 0.64444444444444449)\n",
      "(24, 0.64444444444444449)\n",
      "(23, 0.62222222222222223)\n",
      "(22, 0.64444444444444449)\n",
      "(21, 0.62222222222222223)\n",
      "(20, 0.66666666666666663)\n",
      "(19, 0.64444444444444449)\n",
      "(18, 0.64444444444444449)\n",
      "(17, 0.62222222222222223)\n",
      "(16, 0.64444444444444449)\n",
      "(15, 0.59999999999999998)\n",
      "(14, 0.66666666666666663)\n",
      "(13, 0.59999999999999998)\n",
      "(12, 0.62222222222222223)\n",
      "(11, 0.66666666666666663)\n",
      "(10, 0.71111111111111114)\n",
      "(9, 0.66666666666666663)\n",
      "(8, 0.71111111111111114)\n",
      "(7, 0.66666666666666663)\n",
      "(6, 0.64444444444444449)\n",
      "(5, 0.66666666666666663)\n",
      "(4, 0.66666666666666663)\n",
      "(3, 0.68888888888888888)\n",
      "(2, 0.64444444444444449)\n",
      "(1, 0.71111111111111114)\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,30)[::-1]:\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    print(k, knn.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Scott Foreman-Roe's [breakdown](http://scott.fortmann-roe.com/docs/BiasVariance.html) (required) of the bias-variance tradeoff featuring a discussion of KNN is an excellent read\n",
    "- A [detailed discussion](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) of KNN\n",
    "- A long, applied example of KNN applied to [image classification](http://cs231n.github.io/classification/ )\n",
    "- If academic breakdowns are your thing, be sure to visit [this](http://me.seekingqed.com/files/intro_KNN.pdf) resource\n",
    "- Read the SKLearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on implementing KNN\n",
    "- Choosing the right [algorithm from SKLearn](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- A deeper dive into [Euclidian distance](http://www.econ.upf.edu/~michael/stanford/maeb4.pdf)\n",
    "- Classifier comparsion from [SKLearn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) (this is also in our [repository](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-04/4.01%20Intro%20to%20Classification/classification-methods.py))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
